{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f806858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load basic dependencies:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.models import resnet50, resnet18, alexnet\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "\n",
    "# Load ART dependencies:\n",
    "# from art.estimators.classification import KerasClassifier\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.attacks.evasion import ProjectedGradientDescent, ShadowAttack, CarliniL2Method, FeatureAdversariesPyTorch, DeepFool, AutoProjectedGradientDescent\n",
    "from art.defences.preprocessor import SpatialSmoothing\n",
    "from art.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a0e7908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ec5a54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./data/resnet18_imagenet100.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f4bf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d843176",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    clip_values=(0, 255),\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(3, 224, 224),  # ResNet18 expects input shape (C, H, W)\n",
    "    nb_classes=100,\n",
    "    #preprocessing=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc96a0",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917db226",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "008c07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train_dataset to numpy arrays (images and labels)\n",
    "def convert_to_numpy(dataset):\n",
    "    images_np = []\n",
    "    labels_np = []\n",
    "\n",
    "    for img, label in tqdm(dataset):\n",
    "        # img is a torch.Tensor (C, H, W), convert to numpy and transpose to (H, W, C)\n",
    "        #img_np = img.numpy().transpose(1, 2, 0)\n",
    "        images_np.append(img)\n",
    "        labels_np.append(label)\n",
    "\n",
    "    images_np = np.stack(images_np)\n",
    "    labels_np = np.array(labels_np)\n",
    "\n",
    "    print('Images shape:', images_np.shape)\n",
    "    print('Labels shape:', labels_np.shape)\n",
    "    return images_np, labels_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e5960f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do it with dataoader torch, and remove the previous function\n",
    "def generate_adv_batch(images, adv, batch_size=32, labels=None):\n",
    "    img_adv=[]\n",
    "    for i in tqdm(range(0, len(images), batch_size)):\n",
    "        batch_images = images[i:i+batch_size]\n",
    "        if labels is None:\n",
    "            x_adv = adv.generate(batch_images)\n",
    "        else:\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            # x_adv = adv.generate(batch_images, y=to_categorical(batch_labels, nb_classes=100))\n",
    "            x_adv = adv.generate(batch_images, y=batch_labels)\n",
    "\n",
    "        img_adv.append(x_adv)\n",
    "\n",
    "    img_adv = np.concatenate(img_adv, axis=0)\n",
    "    return img_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "91715f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(og_img_list, adv_img_list, labels, save_dir):\n",
    "    \"\"\"Create dataset for discriminate original - adv images\"\"\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    og_dir = os.path.join(save_dir, \"og\")\n",
    "    og_np_dir = os.path.join(save_dir, \"og_np\")\n",
    "    adv_dir = os.path.join(save_dir, \"adv\")\n",
    "    adv_np_dir = os.path.join(save_dir, \"adv_np\")\n",
    "    os.makedirs(og_np_dir, exist_ok=True)\n",
    "    os.makedirs(adv_np_dir, exist_ok=True)\n",
    "    os.makedirs(og_dir, exist_ok=True)\n",
    "    os.makedirs(adv_dir, exist_ok=True)\n",
    "\n",
    "    assert og_img_list.shape[0] == adv_img_list.shape[0], \"Original and adversarial images must have the same number of samples.\"\n",
    "\n",
    "    for i in tqdm(range(og_img_list.shape[0])):\n",
    "        # Convert (C, H, W) to (H, W, C) and scale to [0,255]\n",
    "        og_img = og_img_list[i].transpose(1, 2, 0)\n",
    "        adv_img = adv_img_list[i].transpose(1, 2, 0)\n",
    "        og_img = np.clip(og_img, 0, 1)\n",
    "        adv_img = np.clip(adv_img, 0, 1)\n",
    "        og_img = (og_img * 255).astype(np.uint8)\n",
    "        adv_img = (adv_img * 255).astype(np.uint8)\n",
    "        og_img_pil = PILImage.fromarray(og_img)\n",
    "        adv_img_pil = PILImage.fromarray(adv_img)\n",
    "        og_img_pil.save(os.path.join(og_dir, f\"{i:05d}.png\"))\n",
    "        adv_img_pil.save(os.path.join(adv_dir, f\"{i:05d}.png\"))\n",
    "\n",
    "        np.save(os.path.join(og_np_dir, f\"{i:05d}.npy\"), og_img_list[i])\n",
    "        np.save(os.path.join(adv_np_dir, f\"{i:05d}.npy\"), adv_img_list[i])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    labels_path = os.path.join(save_dir, \"labels_og_adv.npy\")\n",
    "    np.save(labels_path, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e23277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, optimizer, criterion, device, train_loader, num_epochs):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc6842",
   "metadata": {},
   "source": [
    "## Create and Save dataset\n",
    "Note: Images are normalized (imgnet meand and std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1e79ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "transform = ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "dataset_path = '/mnt/ssd1t/datasets/imagenet_100'\n",
    "dataset = ImageFolder(root=dataset_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b3e90b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:24<00:00, 204.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (5000, 3, 224, 224)\n",
      "Labels shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "images_np, labels_np = convert_to_numpy(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cef46549",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_np_subset = images_np[:10]\n",
    "labels_np_subset = labels_np[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28eff01",
   "metadata": {},
   "source": [
    "### PDG Untargeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e09c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = ProjectedGradientDescent(classifier, targeted=False, batch_size=32, max_iter=20, eps_step=0.001, eps=5, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [07:30<00:00,  2.87s/it]\n"
     ]
    }
   ],
   "source": [
    "images_adv_np = generate_adv_batch(images_np, adv, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc36de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:30<00:00, 33.18it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(images_np, images_adv_np, labels_np, \"/mnt/ssd1t/datasets/imagenet_100_adv/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44816a6",
   "metadata": {},
   "source": [
    "### PGD Targeted (random label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = ProjectedGradientDescent(classifier, targeted=True, batch_size=32, max_iter=20, eps_step=0.001, eps=5, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_labels_np = np.random.randint(0, 100, size=labels_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f6f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [07:12<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "images_adv_np = generate_adv_batch(images_np, adv, batch_size=32, labels=random_labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc20cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:26<00:00, 34.04it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(images_np, images_adv_np, labels_np, \"/mnt/ssd1t/datasets/imagenet_100_adv_random_targeted/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb4851",
   "metadata": {},
   "source": [
    "### Shadow Attack Untargeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = ShadowAttack(classifier, targeted=False, sigma=0.01, nb_steps=1000, learning_rate=0.2, lambda_tv=0.3, lambda_c=1.0, lambda_s=0.5, batch_size=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "40642996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.50s/it]\n"
     ]
    }
   ],
   "source": [
    "images_adv_np = generate_adv_batch(images_np, adv, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f06f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 40.57it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(images_np, images_adv_np, labels_np, \"/mnt/ssd1t/datasets/imagenet_100_adv_shadow/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d0911",
   "metadata": {},
   "source": [
    "### Carlini L2 Untargeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = CarliniL2Method(classifier, confidence=0.0, targeted=False, learning_rate=0.01, \n",
    "                      binary_search_steps=10, max_iter=10, initial_const=0.01, max_halving=5, max_doubling=5, batch_size=1  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f326c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C&W L_2: 100%|██████████| 1/1 [02:10<00:00, 130.41s/it]\n",
      "C&W L_2: 100%|██████████| 1/1 [01:52<00:00, 112.04s/it]\n",
      "C&W L_2: 100%|██████████| 1/1 [01:51<00:00, 111.92s/it]\n",
      "C&W L_2: 100%|██████████| 1/1 [02:05<00:00, 125.72s/it]\n",
      "C&W L_2: 100%|██████████| 1/1 [01:42<00:00, 102.47s/it]\n",
      "C&W L_2: 100%|██████████| 1/1 [01:31<00:00, 91.70s/it]\n",
      "C&W L_2: 100%|██████████| 1/1 [00:22<00:00, 22.93s/it]\n",
      "100%|██████████| 7/7 [11:37<00:00, 99.69s/it] \n"
     ]
    }
   ],
   "source": [
    "images_adv_np = generate_adv_batch(images_np_subset, adv, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d65d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 41.35it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(images_np_subset, images_adv_np, labels_np_subset, \"/mnt/ssd1t/datasets/imagenet_100_adv_carlini_l2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b26e3",
   "metadata": {},
   "source": [
    "### FeatureAdversariesPyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b654762e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_np.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52d0a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_image = images_np[496]\n",
    "guide_images_np = np.stack([guide_image] * images_np_subset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f5edab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = FeatureAdversariesPyTorch(classifier, delta=1.0, optimizer=None, lambda_=0.0, layer=-1, max_iter=100, batch_size=8, step_size=0.01, random_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f774a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:07<00:00, 13.26it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:06<00:00, 15.29it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:06<00:00, 15.26it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:06<00:00, 15.24it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:06<00:00, 15.21it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:06<00:00, 15.20it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:06<00:00, 15.20it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:06<00:00, 15.19it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:06<00:00, 15.19it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:06<00:00, 15.17it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:06<00:00, 15.17it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:06<00:00, 15.14it/s]\n",
      "Feature Adversaries PyTorch: 100%|██████████| 100/100 [00:04<00:00, 24.79it/s]\n",
      "100%|██████████| 13/13 [01:24<00:00,  6.51s/it]\n"
     ]
    }
   ],
   "source": [
    "images_adv_np = generate_adv_batch(images_np_subset, adv, batch_size=8, labels=guide_images_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc12cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 42.07it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(images_np_subset, images_adv_np, labels_np_subset, \"/mnt/ssd1t/datasets/imagenet_100_adv_FA/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58552da",
   "metadata": {},
   "source": [
    "### DeepFool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eecaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = DeepFool(classifier, max_iter=100, epsilon=1e-6, nb_grads=20, batch_size=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae2450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:07<00:00,  2.60s/it]\n"
     ]
    }
   ],
   "source": [
    "images_adv_np = generate_adv_batch(images_np_subset, adv, batch_size=1, labels=labels_np_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6b64eeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 33.49it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(images_np_subset, images_adv_np, labels_np_subset, \"/mnt/ssd1t/datasets/imagenet_100_adv_deepfool/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7de420",
   "metadata": {},
   "source": [
    "### Auto PGD Untargeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = AutoProjectedGradientDescent(classifier, norm=\"inf\", eps=0.3, eps_step=0.1, max_iter=100, \n",
    "                                   targeted=False, nb_random_init=5, batch_size=1, loss_type=\"cross_entropy\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "855129fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "AutoPGD - restart:  20%|██        | 1/5 [00:01<00:06,  1.63s/it]\n",
      " 10%|█         | 1/10 [00:01<00:14,  1.64s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "AutoPGD - restart:  20%|██        | 1/5 [00:01<00:06,  1.71s/it]\n",
      " 20%|██        | 2/10 [00:03<00:13,  1.69s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "AutoPGD - restart:  20%|██        | 1/5 [00:01<00:07,  1.76s/it]\n",
      " 30%|███       | 3/10 [00:05<00:12,  1.72s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "AutoPGD - restart:  20%|██        | 1/5 [00:01<00:06,  1.61s/it]\n",
      " 40%|████      | 4/10 [00:06<00:10,  1.68s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "AutoPGD - restart:  20%|██        | 1/5 [00:01<00:06,  1.69s/it]\n",
      " 50%|█████     | 5/10 [00:08<00:08,  1.69s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "AutoPGD - restart:  20%|██        | 1/5 [00:01<00:06,  1.74s/it]\n",
      " 60%|██████    | 6/10 [00:10<00:06,  1.71s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "AutoPGD - restart:  20%|██        | 1/5 [00:01<00:06,  1.61s/it]\n",
      " 70%|███████   | 7/10 [00:11<00:05,  1.68s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "AutoPGD - restart:  20%|██        | 1/5 [00:01<00:06,  1.68s/it]\n",
      " 80%|████████  | 8/10 [00:13<00:03,  1.68s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "AutoPGD - restart:  20%|██        | 1/5 [00:01<00:06,  1.69s/it]\n",
      " 90%|█████████ | 9/10 [00:15<00:01,  1.69s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "AutoPGD - restart:  20%|██        | 1/5 [00:01<00:06,  1.61s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "images_adv_np = generate_adv_batch(images_np_subset, adv, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ff146b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 39.15it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(images_np_subset, images_adv_np, labels_np_subset, \"/mnt/ssd1t/datasets/imagenet_100_adv_autopgd/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44538982",
   "metadata": {},
   "source": [
    "# Train with adv images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a78d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithNumpyLabels(Dataset):\n",
    "    def __init__(self, images_dir, labels_path=None, label=None, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.label = label\n",
    "        self.image_files = sorted(os.listdir(images_dir))\n",
    "        if labels_path:\n",
    "            self.labels = np.load(labels_path)\n",
    "            assert len(self.image_files) == len(self.labels), \"Number of images and labels must match\"\n",
    "       \n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) #len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_np = np.load(os.path.join(self.images_dir, self.image_files[idx]))\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        if self.label is not None:\n",
    "            label = self.label\n",
    "        else:\n",
    "            label = int(self.labels[idx])\n",
    "        \n",
    "        return torch.Tensor(img_np), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6483f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"/mnt/ssd1t/datasets/imagenet_100_adv_carlini_l2/\"\n",
    "#img_targeted_dir = \"/mnt/ssd1t/datasets/imagenet_100_adv_random_targeted/\"\n",
    "labels_path = \"/mnt/ssd1t/datasets/imagenet_100_adv_carlini_l2/labels_og_adv.npy\"\n",
    "\n",
    "dataset_og_norm = ImageFolderWithNumpyLabels(os.path.join(images_dir, 'og_np'), labels_path)\n",
    "dataset_adv_norm = ImageFolderWithNumpyLabels(os.path.join(images_dir, 'adv_np'), labels_path)\n",
    "#dataset_adv_norm_targeted = ImageFolderWithNumpyLabels(os.path.join(img_targeted_dir, 'adv_np'), labels_path)\n",
    "\n",
    "train_size = int(0.8 * len(dataset_og_norm))\n",
    "val_size = len(dataset_og_norm) - train_size\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataset_og, val_dataset_og = random_split(dataset_og_norm, [train_size, val_size])\n",
    "torch.manual_seed(42)\n",
    "train_dataset_adv, val_dataset_adv = random_split(dataset_adv_norm, [train_size, val_size])\n",
    "#torch.manual_seed(42)\n",
    "#train_dataset_adv_targeted, val_dataset_adv_targeted = random_split(dataset_adv_norm_targeted, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20b927d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = ConcatDataset([train_dataset_og, train_dataset_adv])\n",
    "#train_aug = ConcatDataset([train_dataset_og, train_dataset_adv, train_dataset_adv_targeted])\n",
    "train_loader_aug = DataLoader(train_aug, batch_size=32, shuffle=True)\n",
    "val_aug = ConcatDataset([val_dataset_adv, val_dataset_og])\n",
    "val_loader_aug = DataLoader(val_aug, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "462df272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.2866 - Accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 0.2486 - Accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 0.2473 - Accuracy: 0.9812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 0.2182 - Accuracy: 0.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Loss: 0.2000 - Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Loss: 0.1591 - Accuracy: 0.9938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Loss: 0.1699 - Accuracy: 0.9812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 0.1378 - Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Loss: 0.1177 - Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Loss: 0.1378 - Accuracy: 0.9812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-4)\n",
    "model = train_classifier(model, optimizer, criterion, device, train_loader_aug, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ca414da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = \"./data/resnet18_imagenet100_adv_carlini_l2_mix.pth\"\n",
    "# model_path = \"./data/resnet18_imagenet100_adv_FA_mix.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4640a924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = \"./data/resnet18_imagenet100_adv_FA_mix.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7097b22",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad80730",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3becef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, wrapper, test_dataset_og, test_dataset_adv):\n",
    "    test_dataloader = DataLoader(test_dataset_og, batch_size=32, shuffle=False)\n",
    "    test_adv_np, test_lbl_np = convert_to_numpy(test_dataset_adv)\n",
    "    test_dataset_aug = ConcatDataset([test_dataset_og, test_dataset_adv])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad44236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(val_dataset_og, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b7ac71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49efa9b",
   "metadata": {},
   "source": [
    "## adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58d6d08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 4006.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (20, 3, 224, 224)\n",
      "Labels shape: (20,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_images_np, val_lbl_np = convert_to_numpy(val_dataset_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9a58c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of adversarial samples: 0.95\n"
     ]
    }
   ],
   "source": [
    "pred_adv = classifier.predict(val_images_np)\n",
    "label_adv = np.argmax(pred_adv, axis=1)\n",
    "acc = np.sum(label_adv == val_lbl_np) / val_lbl_np.shape[0]\n",
    "print('Accuracy of adversarial samples:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc40ed",
   "metadata": {},
   "source": [
    "## og + adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0d4efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 4872.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (40, 3, 224, 224)\n",
      "Labels shape: (40,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_images_aug_np, val_lbl_aug_np = convert_to_numpy(val_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8f43cf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of adversarial samples: 0.975\n"
     ]
    }
   ],
   "source": [
    "pred_adv = classifier.predict(val_images_aug_np)\n",
    "label_adv = np.argmax(pred_adv, axis=1)\n",
    "acc = np.sum(label_adv == val_lbl_aug_np) / val_lbl_aug_np.shape[0]\n",
    "print('Accuracy of adversarial samples:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38cc7e",
   "metadata": {},
   "source": [
    "# Module to predict adv images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"/mnt/ssd1t/datasets/imagenet_100_adv/\"\n",
    "labels_path = \"/mnt/ssd1t/datasets/imagenet_100_adv/labels_og_adv.npy\"\n",
    "\n",
    "dataset_og_norm = ImageFolderWithNumpyLabels(os.path.join(images_dir, 'og_np'), label=0)\n",
    "dataset_adv_norm = ImageFolderWithNumpyLabels(os.path.join(images_dir, 'adv_np'), label=1)\n",
    "\n",
    "train_size = int(0.8 * len(dataset_og_norm))\n",
    "val_size = len(dataset_og_norm) - train_size\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataset_og, val_dataset_og = random_split(dataset_og_norm, [train_size, val_size])\n",
    "torch.manual_seed(42)\n",
    "train_dataset_adv, val_dataset_adv = random_split(dataset_adv_norm, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = ConcatDataset([train_dataset_og, train_dataset_adv])\n",
    "train_loader_aug = DataLoader(train_aug, batch_size=32, shuffle=True)\n",
    "val_aug = ConcatDataset([val_dataset_og, val_dataset_adv])\n",
    "val_loader_aug = DataLoader(val_aug, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5dad2",
   "metadata": {},
   "source": [
    "## Train naive model (resnet pretrained on imagenet-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=False)\n",
    "#model_path = \"./resnet18_imagenet100.pth\"\n",
    "#model.fc = nn.Linear(model.fc.in_features, 100)\n",
    "#model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the classifier (head)\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"fc\"):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fff8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.4800 - Accuracy: 0.7941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 0.3801 - Accuracy: 0.8424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:14<00:00, 17.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 0.3597 - Accuracy: 0.8465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 0.3406 - Accuracy: 0.8535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Loss: 0.3327 - Accuracy: 0.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Loss: 0.3204 - Accuracy: 0.8661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Loss: 0.3073 - Accuracy: 0.8759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 0.3028 - Accuracy: 0.8726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Loss: 0.2977 - Accuracy: 0.8779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Loss: 0.2890 - Accuracy: 0.8801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader_aug):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434df001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 5387.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (2000, 3, 224, 224)\n",
      "Labels shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "val_images_aug_np, val_lbl_aug_np = convert_to_numpy(val_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    clip_values=(0, 255),\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(3, 224, 224),  # ResNet18 expects input shape (C, H, W)\n",
    "    nb_classes=2,\n",
    "    #preprocessing=preprocessor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434524ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "Accuracy of adversarial samples: 0.8885\n"
     ]
    }
   ],
   "source": [
    "pred_adv = classifier.predict(val_images_aug_np)\n",
    "print(pred_adv.shape)\n",
    "label_adv = np.argmax(pred_adv, axis=1)\n",
    "acc = np.sum(label_adv == val_lbl_aug_np) / val_lbl_aug_np.shape[0]\n",
    "print('Accuracy of adversarial samples:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8149e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
