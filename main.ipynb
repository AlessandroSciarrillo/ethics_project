{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f806858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load basic dependencies:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.models import resnet50, resnet18, alexnet\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "\n",
    "# Load ART dependencies:\n",
    "# from art.estimators.classification import KerasClassifier\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "from art.defences.preprocessor import SpatialSmoothing\n",
    "from art.utils import to_categorical\n",
    "\n",
    "# Install ImageNet stubs:\n",
    "#!{sys.executable} -m pip install git+https://github.com/nottombrown/imagenet_stubs\n",
    "import imagenet_stubs\n",
    "from imagenet_stubs.imagenet_2012_labels import name_to_label, label_to_name\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc96a0",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917db226",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008c07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train_dataset to numpy arrays (images and labels)\n",
    "def convert_to_numpy(dataset):\n",
    "    images_np = []\n",
    "    labels_np = []\n",
    "\n",
    "    for img, label in tqdm(dataset):\n",
    "        # img is a torch.Tensor (C, H, W), convert to numpy and transpose to (H, W, C)\n",
    "        #img_np = img.numpy().transpose(1, 2, 0)\n",
    "        images_np.append(img)\n",
    "        labels_np.append(label)\n",
    "\n",
    "    images_np = np.stack(images_np)\n",
    "    labels_np = np.array(labels_np)\n",
    "\n",
    "    print('Images shape:', images_np.shape)\n",
    "    print('Labels shape:', labels_np.shape)\n",
    "    return images_np, labels_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5960f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do it with dataoader torch, and remove the previous function\n",
    "def generate_adv_batch(images, adv, batch_size=32, labels=None):\n",
    "    img_adv=[]\n",
    "    for i in tqdm(range(0, len(images), batch_size)):\n",
    "        batch_images = images[i:i+batch_size]\n",
    "        if labels is None:\n",
    "            x_adv = adv.generate(batch_images)\n",
    "        else:\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            x_adv = adv.generate(batch_images, y=to_categorical(batch_labels, nb_classes=100))\n",
    "        img_adv.append(x_adv)\n",
    "\n",
    "    img_adv = np.concatenate(img_adv, axis=0)\n",
    "    return img_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91715f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(og_img_list, adv_img_list, labels, save_dir):\n",
    "    \"\"\"Create dataset for discriminate original - adv images\"\"\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    og_dir = os.path.join(save_dir, \"og\")\n",
    "    og_np_dir = os.path.join(save_dir, \"og_np\")\n",
    "    adv_dir = os.path.join(save_dir, \"adv\")\n",
    "    adv_np_dir = os.path.join(save_dir, \"adv_np\")\n",
    "    os.makedirs(og_np_dir, exist_ok=True)\n",
    "    os.makedirs(adv_np_dir, exist_ok=True)\n",
    "    os.makedirs(og_dir, exist_ok=True)\n",
    "    os.makedirs(adv_dir, exist_ok=True)\n",
    "\n",
    "    assert og_img_list.shape[0] == adv_img_list.shape[0], \"Original and adversarial images must have the same number of samples.\"\n",
    "\n",
    "    for i in tqdm(range(og_img_list.shape[0])):\n",
    "        # Convert (C, H, W) to (H, W, C) and scale to [0,255]\n",
    "        og_img = og_img_list[i].transpose(1, 2, 0)\n",
    "        adv_img = adv_img_list[i].transpose(1, 2, 0)\n",
    "        og_img = np.clip(og_img, 0, 1)\n",
    "        adv_img = np.clip(adv_img, 0, 1)\n",
    "        og_img = (og_img * 255).astype(np.uint8)\n",
    "        adv_img = (adv_img * 255).astype(np.uint8)\n",
    "        og_img_pil = PILImage.fromarray(og_img)\n",
    "        adv_img_pil = PILImage.fromarray(adv_img)\n",
    "        og_img_pil.save(os.path.join(og_dir, f\"{i:05d}.png\"))\n",
    "        adv_img_pil.save(os.path.join(adv_dir, f\"{i:05d}.png\"))\n",
    "\n",
    "        np.save(os.path.join(og_np_dir, f\"{i:05d}.npy\"), og_img_list[i])\n",
    "        np.save(os.path.join(adv_np_dir, f\"{i:05d}.npy\"), adv_img_list[i])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    labels_path = os.path.join(save_dir, \"labels_og_adv.npy\")\n",
    "    np.save(labels_path, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc6842",
   "metadata": {},
   "source": [
    "## Create and Save dataset\n",
    "Note: Images are normalized (imgnet meand and std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e79ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "transform = ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "dataset_path = '/mnt/ssd1t/datasets/imagenet_100'\n",
    "dataset = ImageFolder(root=dataset_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b87f2b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090a5974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./resnet18_imagenet100.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc7f5399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e90b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:27<00:00, 184.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (5000, 3, 224, 224)\n",
      "Labels shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "images_np, labels_np = convert_to_numpy(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06adac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    clip_values=(0, 255),\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(3, 224, 224),  # ResNet18 expects input shape (C, H, W)\n",
    "    nb_classes=100,\n",
    "    #preprocessing=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28eff01",
   "metadata": {},
   "source": [
    "### Untargeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3e09c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = ProjectedGradientDescent(classifier, targeted=False, batch_size=32, max_iter=20, eps_step=0.001, eps=5, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [07:30<00:00,  2.87s/it]\n"
     ]
    }
   ],
   "source": [
    "images_adv_np = generate_adv_batch(images_np, adv, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19fc36de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:30<00:00, 33.18it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(images_np, images_adv_np, labels_np, \"/mnt/ssd1t/datasets/imagenet_100_adv/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44816a6",
   "metadata": {},
   "source": [
    "### Targeted (random label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8596bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = ProjectedGradientDescent(classifier, targeted=True, batch_size=32, max_iter=20, eps_step=0.001, eps=5, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f31d8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_labels_np = np.random.randint(0, 100, size=labels_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d23f6f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [07:12<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "images_adv_np = generate_adv_batch(images_np, adv, batch_size=32, labels=random_labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc20cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:26<00:00, 34.04it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(images_np, images_adv_np, labels_np, \"/mnt/ssd1t/datasets/imagenet_100_adv_random_targeted/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44538982",
   "metadata": {},
   "source": [
    "# Train with adv images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a78d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithNumpyLabels(Dataset):\n",
    "    def __init__(self, images_dir, labels_path=None, label=None, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.label = label\n",
    "        self.image_files = sorted(os.listdir(images_dir))\n",
    "        if labels_path:\n",
    "            self.labels = np.load(labels_path)\n",
    "            assert len(self.image_files) == len(self.labels), \"Number of images and labels must match\"\n",
    "       \n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) #len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_np = np.load(os.path.join(self.images_dir, self.image_files[idx]))\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        if self.label is not None:\n",
    "            label = self.label\n",
    "        else:\n",
    "            label = int(self.labels[idx])\n",
    "        \n",
    "        return torch.Tensor(img_np), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6483f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"/mnt/ssd1t/datasets/imagenet_100_adv_random_targeted/\"\n",
    "labels_path = \"/mnt/ssd1t/datasets/imagenet_100_adv_old/labels_og_adv.npy\"\n",
    "\n",
    "dataset_og_norm = ImageFolderWithNumpyLabels(os.path.join(images_dir, 'og_np'), labels_path)\n",
    "dataset_adv_norm = ImageFolderWithNumpyLabels(os.path.join(images_dir, 'adv_np'), labels_path)\n",
    "\n",
    "train_size = int(0.8 * len(dataset_og_norm))\n",
    "val_size = len(dataset_og_norm) - train_size\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataset_og, val_dataset_og = random_split(dataset_og_norm, [train_size, val_size])\n",
    "torch.manual_seed(42)\n",
    "train_dataset_adv, val_dataset_adv = random_split(dataset_adv_norm, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b927d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = ConcatDataset([train_dataset_og, train_dataset_adv])\n",
    "train_loader_aug = DataLoader(train_aug, batch_size=32, shuffle=True)\n",
    "val_aug = ConcatDataset([val_dataset_og, val_dataset_adv])\n",
    "val_loader_aug = DataLoader(val_aug, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "462df272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:47<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 2.1209 - Accuracy: 0.5634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:35<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 1.8404 - Accuracy: 0.6471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:35<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 1.6948 - Accuracy: 0.6799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:35<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 1.5889 - Accuracy: 0.6993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:34<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Loss: 1.4992 - Accuracy: 0.7238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:35<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Loss: 1.4211 - Accuracy: 0.7362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:34<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Loss: 1.3596 - Accuracy: 0.7458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:35<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 1.2888 - Accuracy: 0.7630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:34<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Loss: 1.2352 - Accuracy: 0.7770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:34<00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Loss: 1.1851 - Accuracy: 0.7849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader_aug):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ca414da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./resnet18_imagenet100_adv_targeted.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4640a924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7097b22",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad80730",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad44236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(val_dataset_og, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b7ac71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:03<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49efa9b",
   "metadata": {},
   "source": [
    "## adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58d6d08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 392.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (1000, 3, 224, 224)\n",
      "Labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "val_images_np, val_lbl_np = convert_to_numpy(val_dataset_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a58c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of adversarial samples: 0.562\n"
     ]
    }
   ],
   "source": [
    "pred_adv = classifier.predict(val_images_np)\n",
    "label_adv = np.argmax(pred_adv, axis=1)\n",
    "acc = np.sum(label_adv == val_lbl_np) / val_lbl_np.shape[0]\n",
    "print('Accuracy of adversarial samples:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc40ed",
   "metadata": {},
   "source": [
    "## og + adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0d4efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 3994.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (2000, 3, 224, 224)\n",
      "Labels shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "val_images_aug_np, val_lbl_aug_np = convert_to_numpy(val_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f43cf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of adversarial samples: 0.643\n"
     ]
    }
   ],
   "source": [
    "pred_adv = classifier.predict(val_images_aug_np)\n",
    "label_adv = np.argmax(pred_adv, axis=1)\n",
    "acc = np.sum(label_adv == val_lbl_aug_np) / val_lbl_aug_np.shape[0]\n",
    "print('Accuracy of adversarial samples:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38cc7e",
   "metadata": {},
   "source": [
    "# Module to predict adv images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44cc164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"/mnt/ssd1t/datasets/imagenet_100_adv/\"\n",
    "labels_path = \"/mnt/ssd1t/datasets/imagenet_100_adv/labels_og_adv.npy\"\n",
    "\n",
    "dataset_og_norm = ImageFolderWithNumpyLabels(os.path.join(images_dir, 'og_np'), label=0)\n",
    "dataset_adv_norm = ImageFolderWithNumpyLabels(os.path.join(images_dir, 'adv_np'), label=1)\n",
    "\n",
    "train_size = int(0.8 * len(dataset_og_norm))\n",
    "val_size = len(dataset_og_norm) - train_size\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataset_og, val_dataset_og = random_split(dataset_og_norm, [train_size, val_size])\n",
    "torch.manual_seed(42)\n",
    "train_dataset_adv, val_dataset_adv = random_split(dataset_adv_norm, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff2742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = ConcatDataset([train_dataset_og, train_dataset_adv])\n",
    "train_loader_aug = DataLoader(train_aug, batch_size=32, shuffle=True)\n",
    "val_aug = ConcatDataset([val_dataset_og, val_dataset_adv])\n",
    "val_loader_aug = DataLoader(val_aug, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5dad2",
   "metadata": {},
   "source": [
    "## Train naive model (resnet pretrained on imagenet-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76c8968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=False)\n",
    "#model_path = \"./resnet18_imagenet100.pth\"\n",
    "#model.fc = nn.Linear(model.fc.in_features, 100)\n",
    "#model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f0fb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the classifier (head)\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"fc\"):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25fff8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.4800 - Accuracy: 0.7941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 0.3801 - Accuracy: 0.8424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:14<00:00, 17.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 0.3597 - Accuracy: 0.8465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 0.3406 - Accuracy: 0.8535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Loss: 0.3327 - Accuracy: 0.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Loss: 0.3204 - Accuracy: 0.8661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Loss: 0.3073 - Accuracy: 0.8759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 0.3028 - Accuracy: 0.8726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Loss: 0.2977 - Accuracy: 0.8779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Loss: 0.2890 - Accuracy: 0.8801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader_aug):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "434df001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 5387.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (2000, 3, 224, 224)\n",
      "Labels shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "val_images_aug_np, val_lbl_aug_np = convert_to_numpy(val_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb22cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    clip_values=(0, 255),\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(3, 224, 224),  # ResNet18 expects input shape (C, H, W)\n",
    "    nb_classes=2,\n",
    "    #preprocessing=preprocessor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "434524ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "Accuracy of adversarial samples: 0.8885\n"
     ]
    }
   ],
   "source": [
    "pred_adv = classifier.predict(val_images_aug_np)\n",
    "print(pred_adv.shape)\n",
    "label_adv = np.argmax(pred_adv, axis=1)\n",
    "acc = np.sum(label_adv == val_lbl_aug_np) / val_lbl_aug_np.shape[0]\n",
    "print('Accuracy of adversarial samples:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8149e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
